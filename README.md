- Adding client version for all devices soon.

# Perform on all network devices supported a GPU & VRAM 
cut main task and transmit all tensors to all devices GPU & memory devices
Threading and reconstruct tokens 
Endpoint from server

# for Model LLM or other in further 

# Objectives :
- Only server load Model LLM 
- Only use GPU and Memory from network devices (cumulate performance)
- Optimize delay to render tokens
