# Shared GPU & VRAM from network devices (mobile, android, iphone, laptop, computer, server,...)

Split the main task into several parallel pieces in order to use the resources of all devices on the local network

Finally receive a response as soon as the first tokens are presented via endpoints

# Objectives :
- Only server load Model LLM 
- Only use GPU and Memory from network devices (cumulate performance)
- Optimize delay to render tokens


Supported LLM Model & GPT2

